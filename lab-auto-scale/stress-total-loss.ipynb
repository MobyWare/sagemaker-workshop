{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Scaling Lab\n",
    "This notebook walks you through how to configure and execute auto scaling on a SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from sagemaker.tensorflow.model import TensorFlowPredictor\n",
    "from sagemaker.estimator import Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy or attach to your endpoint\n",
    "\n",
    "The lab has a dependency on the prior lab involving bringing your own TensorFlow script. To get started, we first attach to the existing endpoint from the prior lab. If the endpoint has already been deleted, we re-deploy it based on the name used earlier for the training job.\n",
    "\n",
    "To locate your specific training job, go back to your notebook from the earlier lab and look at the cell output from the `fit` method. It will show you the specific training job name. **Enter that as `ENDPOINT_NAME` in this cell before proceeding**. This ensures we use the same model you trained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVE_INSTANCE_TYPE = 'ml.c5.xlarge'\n",
    "ENDPOINT_NAME = 'sagemaker-tensorflow-scriptmode-2019-04-17-03-52-34-057'\n",
    "TRAINING_JOB_NAME = ENDPOINT_NAME\n",
    "NOT_RUNNING = True\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "if (NOT_RUNNING):\n",
    "    from sagemaker.tensorflow.serving import Model\n",
    "    model = Model(model_data=f's3://{bucket}/{TRAINING_JOB_NAME}/output/model.tar.gz', \n",
    "                  role=f'{role}')\n",
    "    loss_predictor = model.deploy(initial_instance_count=1, \n",
    "                                       instance_type=SERVE_INSTANCE_TYPE)\n",
    "else:\n",
    "    loss_predictor = TensorFlowPredictor(endpoint_name=ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the endpoint is available, prepare a single payload that we will use in the simple stress test. The actual values do not matter, as we are just trying to simulate load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [ 1.05332958, -0.53354753, -0.69436208, -2.21762908, -3.20396808,  1.03539088,\n",
    "  1.20417872, -1.03589941, -0.35095956, -0.01160373, -0.1615418,  -0.20454251,\n",
    " -0.72053914]\n",
    "print(str(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a simple function for making a prediction. Track the elapsed time and return that as seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(payload):\n",
    "    elapsed_time = time.time()\n",
    "    results = loss_predictor.predict(X)\n",
    "    elapsed_time = time.time() - elapsed_time  \n",
    "    prediction = results['predictions'][0][0]\n",
    "    return elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure a single prediction is working against the endpoint before proceeding to \n",
    "generate load for auto scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure auto scaling on your endpoint\n",
    "\n",
    "Follow these steps to configure auto scaling.\n",
    "\n",
    "1. In a new browser tab, navigate to the `Endpoints` section of the SageMaker console. \n",
    "\n",
    "2. Navigate to the details page for the endpoint. \n",
    "\n",
    "3. Under the `Endpoint runtime settings`, select the one and only variant that was created for this endpoint (it is named `All traffic` by default).\n",
    "\n",
    "4. Click on `Configure auto scaling` in the upper right of `Endpoint runtime settings`.\n",
    "\n",
    "5. Under `Variant automatic scaling`, set the maximum number of instances to 2.\n",
    "\n",
    "6. Under `Built in scaling policy`, set the target to track to 2000 for the  `SageMakerVariantInvocationsPerInstance` metric. \n",
    "\n",
    "7. Click `Save` at the bottom of the page.\n",
    "\n",
    "You have now set a threshold that will be used by SageMaker to determine when to add more instances. If it detects more invocations per instance per minute than the threshold, more instances will be added in an attempt to distribute the load and reduce that metric to the target. \n",
    "\n",
    "We have purposely set the threshold to a low number so that we can more easily trigger scaling. In practice, you will need to perform testing and analysis to determine an appropriate trigger and the right number of instances for your workload.\n",
    "\n",
    "See the detailed documentation on SageMaker auto scaling [here](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute stress tests to force auto scaling\n",
    "\n",
    "Now that the endpoint has auto scaling configured, lets drive some inference traffic against the endpoint. We use multiple client threads to drive sufficient volume of requests to trigger SageMaker auto scaling. The number of requests are mapped across a set of threads. Resulting elasped times are summed and returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(max_threads, max_requests):\n",
    "    pool = ThreadPool(max_threads)\n",
    "    bunch_of_x = []\n",
    "    for i in range(max_requests):\n",
    "        bunch_of_x.append(X)\n",
    "    result = pool.map(predict, bunch_of_x)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    elapsedtime = 0\n",
    "    for i in result:\n",
    "        elapsedtime += i\n",
    "    elapsedtime = np.sum(result)\n",
    "    return elapsedtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drive a few short tests\n",
    "We run a few tests to allow us to start seeing invocation metrics in CloudWatch. This will help you visualize how traffic ramps up on your single instance endpoint, and is eventually distributed across a cluster of instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Starting test 0')\n",
    "run_test(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Starting test 1')\n",
    "run_test(10, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Starting test 2')\n",
    "run_test(10, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe auto scaling\n",
    "\n",
    "To trigger auto scaling, kick off one more round of tests. While that is running, read the instructions in the subsequent cell. It explains how to confirm that auto scaling worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Starting test 3')\n",
    "run_test(10, 400000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the endpoint details console, you should still see the `Desired instance count` as `1`, since the scaling threshold has not been reached. \n",
    "\n",
    "This next test will continuously send traffic to the endpoint for about 15 minutes. During this time, we'll see the invocations per instance rise. Invocations per instance will track exactly the same as the total invocations until auto scaling happens, since we only have a single instance to start with. Note that in practice, you would want to start with at least two instances. This ensures you have higher availability by leveraging multiple availability zones.\n",
    "\n",
    "Once auto scaling is triggered, SageMaker will take several minutes to add new instances (in our case, just one). While the auto scaling is happening, the endpoint details console will show you that the new desired instance count has increased to two. There will also be a blue bar at the top of the console indicating that the endpoint is being updated. Eventually that banner turns green and indicates that the `Endpoint was successfully updated.`\n",
    "\n",
    "Once the expanded set of instances is running, click on `Invocation metrics` from the endpoint details console. This takes you to CloudWatch to show graphs of those metrics. Select two metrics: `Invocations` and `InvocationsPerInstance`. Next, click on the `Graphed metrics` tab, and update the `Statistics` to be `Sum`, and the `Period` to be `1 second`. At the top of the chart, set the time period to 30 minutes (using the `custom` drop down).\n",
    "\n",
    "You will now see the total number of invocations continue at the same pace as before, yet the *invocations per instance* will be cut in half, as SageMaker automatically distributes the load ascross the cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling back in (optional)\n",
    "\n",
    "For extra credit, you can observe SageMaker scaling in (reducing the number of instances) the infrastructure. This will take about 15 minutes after the previous traffic generator is complete. At that point, you should see a scale in event. SageMaker detects the invocations per instance is below the threshold, and automatically reduces the number of instances to avoid being over-provisioned. Cool down parameters are available to control how aggressively SageMaker adds or removes instances.\n",
    "\n",
    "To ensure the CloudWatch alarm scale is triggered, there needs to be at least some traffic to have sufficient data points for the alarm. Here we generate a small load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print('Adding a few invocations every 30s for 15 mins')\n",
    "for i in range(30):\n",
    "    run_test(10, 100)\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint\n",
    "Delete the endpoint, which will take down all of the instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(loss_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
