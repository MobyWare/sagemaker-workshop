{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from sagemaker.tensorflow.model import TensorFlowPredictor\n",
    "from sagemaker.estimator import Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy or attach to your endpoint\n",
    "\n",
    "Attach to the existing endpoint from the prior lab, or deploy the endpoint if it had already been deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-355151823911\n",
      "INFO:sagemaker:Creating model with name: sagemaker-tensorflow-serving-2019-04-17-13-07-17-866\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-tensorflow-serving-2019-04-17-13-07-17-866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "SERVE_INSTANCE_TYPE = 'ml.c5.xlarge'\n",
    "ENDPOINT_NAME = 'sagemaker-tensorflow-scriptmode-2019-04-17-03-52-34-057'\n",
    "TRAINING_JOB_NAME = ENDPOINT_NAME\n",
    "NOT_RUNNING = True\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "if (NOT_RUNNING):\n",
    "    from sagemaker.tensorflow.serving import Model\n",
    "    model = Model(model_data=f's3://{bucket}/{TRAINING_JOB_NAME}/output/model.tar.gz', \n",
    "                  role=f'{role}')\n",
    "    loss_predictor = model.deploy(initial_instance_count=1, \n",
    "                                       instance_type=SERVE_INSTANCE_TYPE)\n",
    "else:\n",
    "    loss_predictor = TensorFlowPredictor(endpoint_name=ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare a single payload that we will use in the simple stress test. The actual values do not matter, as we are just trying to simulate load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.05332958, -0.53354753, -0.69436208, -2.21762908, -3.20396808, 1.03539088, 1.20417872, -1.03589941, -0.35095956, -0.01160373, -0.1615418, -0.20454251, -0.72053914]\n"
     ]
    }
   ],
   "source": [
    "X = [ 1.05332958, -0.53354753, -0.69436208, -2.21762908, -3.20396808,  1.03539088,\n",
    "  1.20417872, -1.03589941, -0.35095956, -0.01160373, -0.1615418,  -0.20454251,\n",
    " -0.72053914]\n",
    "print(str(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(payload):\n",
    "    elapsed_time = time.time()\n",
    "    results = loss_predictor.predict(X)\n",
    "    elapsed_time = time.time() - elapsed_time  \n",
    "    prediction = results['predictions'][0][0]\n",
    "    return elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure a single prediction is working against the endpoint before proceeding to \n",
    "generate load for auto scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18019890785217285"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure auto scaling on your endpoint\n",
    "\n",
    "In a new browser tab, navigate to the `Endpoints` section of the SageMaker console. Navigate to the details page for the endpoint. Under the `Endpoint runtime settings`, select the one and only variant that was created for this endpoint (it is named `All traffic` by default).\n",
    "\n",
    "Click on `Configure auto scaling` in the upper right of `Endpoint runtime settings`.\n",
    "\n",
    "Under `Variant automatic scaling`, set the maximum number of instances to 2.\n",
    "\n",
    "Under `Built in scaling policy`, set the target to track to 2000 for the  `SageMakerVariantInvocationsPerInstance` metric. Click `Save` at the bottom of the page.\n",
    "\n",
    "You have now set a threshold that will be used by SageMaker to determine when to add more instances. If it detects more invocations per instance per minute than the threshold, more instances will be added in an attempt to distribute the load and reduce that metric to the target. \n",
    "\n",
    "We have purposely set the threshold to a low number so that we can more easily trigger scaling. In practice, you will need to perform testing and analysis to determine an appropriate trigger and the right number of instances for your workload.\n",
    "\n",
    "See the detailed documentation [here](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-auto-scaling.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute stress tests to force auto scaling\n",
    "\n",
    "Here we simply use multiple client threads to drive sufficient volume of requests to trigger SageMaker auto scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(max_threads, max_requests):\n",
    "    pool = ThreadPool(max_threads)\n",
    "    bunch_of_x = []\n",
    "    for i in range(max_requests):\n",
    "        bunch_of_x.append(X)\n",
    "    result = pool.map(predict, bunch_of_x)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    elapsedtime = 0\n",
    "    for i in result:\n",
    "        elapsedtime += i\n",
    "    elapsedtime = np.sum(result)\n",
    "    return elapsedtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drive a few short tests\n",
    "We run a few tests to allow us to start seeing invocation metrics in CloudWatch. This will help you visualize how traffic ramps up on your single instance endpoint, and is eventually distributed across a cluster of instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test 0\n",
      "CPU times: user 71.8 ms, sys: 8.7 ms, total: 80.5 ms\n",
      "Wall time: 204 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Starting test 0')\n",
    "run_test(5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test 1\n",
      "CPU times: user 1.69 s, sys: 149 ms, total: 1.84 s\n",
      "Wall time: 2.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Starting test 1')\n",
    "run_test(10, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test 1\n",
      "CPU times: user 1.77 s, sys: 132 ms, total: 1.91 s\n",
      "Wall time: 1.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Starting test 2')\n",
    "run_test(10, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe auto scaling\n",
    "In the endpoint details console, you should still see the `Desired instance count` as `1`, since the scaling threshold has not been reached. This next test will continuously send traffic to the endpoint for about 15 minutes. During this time, we'll see the invocations per instance rise. It will track exactly the same as the total invocations.\n",
    "\n",
    "Once auto scaling is triggered, SageMaker will take several minutes to add new instances (in our case, just one). While the auto scaling is happening, the endpoint details console will show you that the new desired instance count has increased to two, and there will be a blue bar at the top of the console indicating that the endpoint is being updated. Eventually that banner turns green and indicates `Endpoint was successfully updated.`\n",
    "\n",
    "Once the expanded set of instances is running, click on `Invocation metrics` from the endpoint details console. This takes you to CloudWatch to show graphs of those metrics. Select `Invocations` and `InvocationsPerInstance`. Click on `Graphed metrics` and update the `Statistics` to be `Sum`, and the `Period` to be `1 second`. At the top of the chart, set the time period to 30 minutes (using the `custom` drop down).\n",
    "\n",
    "You will see the total number of invocations continue at the same pace as before, yet the invocations per instance will be cut in half as SageMaker automatically distributes the load ascross the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test 3\n",
      "CPU times: user 16min 55s, sys: 1min 2s, total: 17min 58s\n",
      "Wall time: 16min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Starting test 3')\n",
    "run_test(10, 400000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling back in\n",
    "\n",
    "For extra credit, you can observe SageMaker scaling in (reducing the number of instances) the infrastructure. This will take about 15 minutes after the previous traffic generator is complete. At that point, you should see a scale in event. SageMaker detects the invocations per instance is below the threshold, and automatically reduces the number of instances to avoid being over-provisioned. Cool down parameters are available to control how aggressively SageMaker adds or removes instances.\n",
    "\n",
    "To ensure the CloudWatch alarm scale is triggered, there needs to be at least some traffic to have sufficient data points for the alarm. Here we generate a small load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding a few invocations every 30s for 15 mins\n",
      "CPU times: user 12.2 s, sys: 2.25 s, total: 14.4 s\n",
      "Wall time: 15min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Adding a few invocations every 30s for 15 mins')\n",
    "for i in range(30):\n",
    "    run_test(10, 100)\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the endpoint\n",
    "Delete the endpoint, which will take down all of the instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(loss_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
