{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World notebook for Amazon SageMaker pipe mode with TensorFlow\n",
    "\n",
    "New data scientists and machine learning engineers have a treasure trove of examples available on the internet to help get started. These examples tend to leverage small public datasets and demonstrate common use cases and approaches. The data in these examples can be downloaded quickly to a training instance and training can be completed from there. However, many customers have large scale datasets for machine learning that make the simple approach of downloading the full dataset prohibitive. Imagine your training algorithm waiting for the download to complete for 100GB of medical images, or 100TB of video.\n",
    "\n",
    "Amazon SageMaker provides Pipe mode for exactly this purpose. Pipe mode lets you establish a channel to your dataset and feed your training algorithm batches of that data at a time. Your training can start quickly, and you can train on an infinite size dataset.\n",
    "\n",
    "While there are several examples available on the use of Pipe mode, it may be difficult to get it working with your specific scenario given the vast number of possible use cases. This notebook provides an end to end example for training with a fairly common combination:\n",
    "\n",
    "1. Custom TensorFlow neural network.\n",
    "2. Script mode using SageMaker's TensorFlow container.\n",
    "3. Pipe mode to incrementally stream data to the neural network.\n",
    "4. Data stored in TFRecords format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple classification dataset\n",
    "\n",
    "For this example, we use a simple numeric dataset that we will use for binary classification. With the focus of this notebook on quickly and easily demonstrating pipe mode, we use a handful of features and limit the number of samples. Feel free to scale it up to see the approach in action on large datasets. To get started, we create a synthetic dataset and split it into train, test, and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (699, 5), Test shape: (200, 5), Val shape: (101, 5)\n",
      "Train target: (699,), Test target: (200,), Val target: (101,)\n",
      "\n",
      "Sample observation: [-0.40218888 -2.29887905 -0.20157193  0.52203548 -1.09193016]\n",
      "Sample target: 0\n"
     ]
    }
   ],
   "source": [
    "NUM_FEATURES = 5\n",
    "NUM_SAMPLES  = 1000\n",
    "NUM_FILES    = 10\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "X1, Y1 = make_classification(n_samples=NUM_SAMPLES, n_features=NUM_FEATURES, n_redundant=0, \n",
    "                             n_informative=1, n_classes=2, n_clusters_per_class=1, \n",
    "                             shuffle=True, class_sep=2.0)\n",
    "\n",
    "# split data into train and test sets\n",
    "seed = 7\n",
    "val_size  = 0.20\n",
    "test_size = 0.10\n",
    "\n",
    "# Give 70% to train\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X1, Y1, test_size=(test_size + val_size), random_state=seed)\n",
    "# Of the remaining 30%, give 2/3 to validation and 1/3 to test\n",
    "X_test, X_val, y_test, y_val     = \\\n",
    "    train_test_split(X_test, y_test, test_size=(test_size / (test_size + val_size)), \n",
    "                     random_state=seed)\n",
    "\n",
    "print('Train shape: {}, Test shape: {}, Val shape: {}'.format(X_train.shape, \n",
    "                                                              X_test.shape, X_val.shape))\n",
    "print('Train target: {}, Test target: {}, Val target: {}'.format(y_train.shape, \n",
    "                                                                 y_test.shape, y_val.shape))\n",
    "print('\\nSample observation: {}\\nSample target: {}'.format(X_test[0], y_test[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = X_train.shape[0]\n",
    "num_val_samples = X_val.shape[0]\n",
    "num_test_samples = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data to TFRecord files\n",
    "\n",
    "Pipe mode supports RecordIO, TFRecord, and TextLine. Here we will use TFRecord format, and for each of train, test, and val, we generate a set of files so we can see how Pipe mode is able to deal with sets of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def _int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def _float_feature(value):\n",
    "  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def convert_to_tfr(x, y, out_file):\n",
    "    with tf.python_io.TFRecordWriter(out_file) as record_writer:\n",
    "      num_samples = len(x)\n",
    "      for i in range(num_samples):\n",
    "        example = tf.train.Example()\n",
    "        example.features.feature['features'].float_list.value.extend(x[i])\n",
    "        example.features.feature['label'].int64_list.value.append(int(y[i]))\n",
    "        record_writer.write(example.SerializeToString())\n",
    "\n",
    "for i in range(NUM_FILES):\n",
    "    convert_to_tfr(X_train, y_train, './data/train/train{}.tfrecords'.format(i))\n",
    "    convert_to_tfr(X_test,  y_test,  './data/test/test{}.tfrecords'.format(i))\n",
    "    convert_to_tfr(X_val,   y_val,   './data/val/val{}.tfrecords'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-355151823911/data/DEMO-hello-pipe-mode\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-hello-pipe-mode'\n",
    "\n",
    "inputs = sagemaker_session.upload_data(path='data', key_prefix='data/DEMO-hello-pipe-mode')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a training job using the `TensorFlow` estimator\n",
    "\n",
    "The `sagemaker.tensorflow.TensorFlow` estimator handles locating the script mode container, uploading your script to a S3 location and creating a SageMaker training job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "train_instance_type = 'ml.c5.xlarge' \n",
    "serve_instance_type = 'ml.m4.xlarge'\n",
    "\n",
    "hyperparameters = {'epochs': 5, 'batch_size': 8, 'model_dir': '/opt/ml/model',\n",
    "                   'num_train_samples': num_train_samples,\n",
    "                   'num_val_samples': num_val_samples,\n",
    "                   'num_test_samples': num_test_samples}\n",
    "\n",
    "pipe_estimator = TensorFlow(entry_point='pipe_train.py',\n",
    "                            source_dir='scripts',\n",
    "                            input_mode='Pipe', #'File',\n",
    "                            train_instance_type=train_instance_type,\n",
    "                            train_instance_count=1,\n",
    "                            metric_definitions=[\n",
    "                               {'Name' : 'validation:acc', \n",
    "                                'Regex': '- val_acc: (.*?$)'},\n",
    "                               {'Name' : 'validation:loss', \n",
    "                                'Regex': '- val_loss: (.*?) '}],\n",
    "                           hyperparameters=hyperparameters,\n",
    "                           role=sagemaker.get_execution_role(),\n",
    "                           framework_version='1.12',\n",
    "                           py_version='py3',\n",
    "                           script_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-24 21:12:02 Starting - Starting the training job.."
     ]
    }
   ],
   "source": [
    "remote_inputs = {'train' : inputs+'/train', \n",
    "                 'val'   : inputs+'/val', \n",
    "                 'test'  : inputs+'/test'}\n",
    "pipe_estimator.fit(remote_inputs, wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
