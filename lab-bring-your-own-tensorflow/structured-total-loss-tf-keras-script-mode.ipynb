{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Total Loss Based on Structured Claim Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use firstname-lastname, or initials, or username\n",
    "# this will be used as a prefix for your training job name, making it easier for you to find the job\n",
    "# later on using the console\n",
    "USER_PREFIX = '<your user name goes here>' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Generate a Dataset\n",
    "\n",
    "We will synthesize a binary classification dataset to represent structured insurance claim data.  We will set two informative features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplots_adjust(bottom=.05, top=.9, left=.05, right=.95)\n",
    "\n",
    "plt.subplot(321)\n",
    "plt.title(\"Two informative features, one cluster per class\", fontsize='small')\n",
    "X1, Y1 = make_classification(n_samples=1000, n_features=13, n_redundant=0, n_informative=2,\n",
    "                             n_classes=2, n_clusters_per_class=1, shuffle=False,\n",
    "                             class_sep=2.0)\n",
    "\n",
    "# scatter plot of the first 2 features, highlighting separation of Loss/Not-Loss classes\n",
    "plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,\n",
    "            s=25, edgecolor='k')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, split the dataset into 3 subsets: training, testing, and validation\n",
    "\n",
    "Here we take the synthetic dataset and create subsets for training, testing, and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# split data into train and test sets\n",
    "seed = 7\n",
    "val_size  = 0.20\n",
    "test_size = 0.10\n",
    "\n",
    "# Give 70% to train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1, Y1, \n",
    "                                                    test_size=(test_size + val_size), \n",
    "                                                    random_state=seed)\n",
    "\n",
    "# Of the remaining 30%, give 2/3 to validation and 1/3 to test\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, \n",
    "                                                test_size=(test_size / (test_size + val_size)),\n",
    "                                                random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, normalize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "mean = X_train.mean(axis=0)\n",
    "X_train -= mean\n",
    "std = X_train.std(axis=0)\n",
    "X_train /= std\n",
    "\n",
    "X_test -= mean\n",
    "X_test /= std\n",
    "\n",
    "X_val -= mean\n",
    "X_val /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the shape of each dataset and show a sample observation and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train shape: {}, Test shape: {}, Val shape: {}'.format(X_train.shape, \n",
    "                                                              X_test.shape, X_val.shape))\n",
    "print('Train target: {}, Test target: {}, Val target: {}'.format(y_train.shape, \n",
    "                                                                 y_test.shape, y_val.shape))\n",
    "\n",
    "print('\\nSample observation: {}\\nSample target: {}'.format(X_test[0], y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the datasets in files to feed the training script\n",
    "\n",
    "Here we use CSV format for files that will be passed to training. We could have just as easily used binary numpy files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "print('Local data dir: {}'.format(data_dir))\n",
    "\n",
    "import pandas as pd\n",
    "xtrain = pd.DataFrame(X_train)\n",
    "xtrain.to_csv(f'{data_dir}/train/xtrain.csv', header=None, index=False)\n",
    "ytrain = pd.DataFrame(y_train)\n",
    "ytrain.to_csv(f'{data_dir}/train/ytrain.csv', header=None, index=False)\n",
    "\n",
    "xtest = pd.DataFrame(X_test)\n",
    "xtest.to_csv(f'{data_dir}/test/xtest.csv', header=None, index=False)\n",
    "ytest = pd.DataFrame(y_test)\n",
    "ytest.to_csv(f'{data_dir}/test/ytest.csv', header=None, index=False)\n",
    "\n",
    "xval = pd.DataFrame(X_val)\n",
    "xval.to_csv(f'{data_dir}/val/xval.csv', header=None, index=False)\n",
    "yval = pd.DataFrame(y_val)\n",
    "yval.to_csv(f'{data_dir}/val/yval.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training script\n",
    "\n",
    "When using Amazon SageMaker's script mode, you provide a training script. In our example, we provide a complete script that you can see below. If you want to experiment with the script, you can edit it directly in the `scripts` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m listdir\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mos.path\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m isfile, join\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow.keras\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m models, layers\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mprep_data\u001b[39;49;00m():\r\n",
      "    base_dir = os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_INPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) + \u001b[33m'\u001b[39;49;00m\u001b[33m/data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "    xtest_df = pd.read_csv(f\u001b[33m'\u001b[39;49;00m\u001b[33m{base_dir}/test/xtest.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, header=\u001b[36mNone\u001b[39;49;00m)\r\n",
      "    xtest = xtest_df.values\r\n",
      "    xtrain_df = pd.read_csv(f\u001b[33m'\u001b[39;49;00m\u001b[33m{base_dir}/train/xtrain.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, header=\u001b[36mNone\u001b[39;49;00m)\r\n",
      "    xtrain = xtrain_df.values\r\n",
      "    xval_df = pd.read_csv(f\u001b[33m'\u001b[39;49;00m\u001b[33m{base_dir}/val/xval.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, header=\u001b[36mNone\u001b[39;49;00m)\r\n",
      "    xval = xval_df.values\r\n",
      "\r\n",
      "    ytest_df = pd.read_csv(f\u001b[33m'\u001b[39;49;00m\u001b[33m{base_dir}/test/ytest.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, header=\u001b[36mNone\u001b[39;49;00m)\r\n",
      "    ytest = ytest_df.values\r\n",
      "    ytrain_df = pd.read_csv(f\u001b[33m'\u001b[39;49;00m\u001b[33m{base_dir}/train/ytrain.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, header=\u001b[36mNone\u001b[39;49;00m)\r\n",
      "    ytrain = ytrain_df.values\r\n",
      "    yval_df = pd.read_csv(f\u001b[33m'\u001b[39;49;00m\u001b[33m{base_dir}/val/yval.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, header=\u001b[36mNone\u001b[39;49;00m)\r\n",
      "    yval = yval_df.values\r\n",
      "\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mxtr: {}, xte: {}, xv: {}, ytr: {}, yte: {}, yv: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(xtrain.shape, \r\n",
      "                                                                      xtest.shape, xval.shape,\r\n",
      "                                                                      ytrain.shape, ytest.shape,\r\n",
      "                                                                      yval.shape))\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m xtrain, xtest, xval, ytrain, ytest, yval\r\n",
      "    \r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m==\u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m20\u001b[39;49;00m)\r\n",
      "    \u001b[37m# input data and model directories\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--val\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_VAL\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "\r\n",
      "    args, _ = parser.parse_known_args()\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33margs: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(args))\r\n",
      "    epochs = args.epochs\r\n",
      "    \r\n",
      "    X_train, X_test, X_val, y_train, y_test, y_val = prep_data()\r\n",
      "    \r\n",
      "    network = models.Sequential()\r\n",
      "    network.add(layers.Dense(\u001b[34m16\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, input_shape=(\u001b[34m13\u001b[39;49;00m,)))\r\n",
      "    network.add(layers.Dense(\u001b[34m16\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    network.add(layers.Dense(\u001b[34m1\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msigmoid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    network.compile(optimizer=\u001b[33m'\u001b[39;49;00m\u001b[33mrmsprop\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, loss=\u001b[33m'\u001b[39;49;00m\u001b[33mbinary_crossentropy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, metrics=[\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\r\n",
      "    network.fit(X_train, y_train, epochs=epochs, batch_size=\u001b[34m100\u001b[39;49;00m,\r\n",
      "                         validation_data=(X_val, y_val))\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mTraining completed.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mEvaluating against test set...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    network.evaluate(x=X_test, y=y_test)\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mSaving model\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    model_version = \u001b[33m'\u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\r\n",
      "    export_dir = os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) + \u001b[33m'\u001b[39;49;00m\u001b[33m/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + \u001b[33m'\u001b[39;49;00m\u001b[33mexport/Servo/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + model_version\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mExport dir: \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m + export_dir)\r\n",
      "\r\n",
      "\u001b[37m#https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md\u001b[39;49;00m\r\n",
      "\u001b[37m#https://towardsdatascience.com/deploying-keras-models-using-tensorflow-serving-and-flask-508ba00f1037\u001b[39;49;00m\r\n",
      "    \r\n",
      "    \u001b[34mwith\u001b[39;49;00m tf.keras.backend.get_session() \u001b[34mas\u001b[39;49;00m sess:\r\n",
      "        tf.saved_model.simple_save(\r\n",
      "            sess,\r\n",
      "            export_dir,\r\n",
      "            inputs={\u001b[33m'\u001b[39;49;00m\u001b[33minput_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: network.input},\r\n",
      "            outputs={\u001b[33m'\u001b[39;49;00m\u001b[33mscore\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: network.output})\r\n",
      "\r\n",
      "\u001b[37m# Detailed calls fail...\u001b[39;49;00m\r\n",
      "\u001b[37m#tensorflow.python.framework.errors_impl.FailedPreconditionError: Error while reading resource variable dense_2_1/bias from Container: localhost. This could mean that the variable was uninitialized. Not found: Container localhost does not exist. (Could not find resource: localhost/dense_2_1/bias)\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mExiting training script.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize 'scripts/loss_train.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a training job using the `TensorFlow` estimator\n",
    "\n",
    "The `sagemaker.tensorflow.TensorFlow` estimator handles locating the script mode container, uploading your script to an S3 location and creating a SageMaker training job. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare to initiate training\n",
    "\n",
    "Here we get ready to kick off the training. We start by using local mode, which executes training directly in the context of your notebook instance. This is a handy way to iteratively build and debug your training script without the overhead of creating separate training infrastructure. \n",
    "\n",
    "For large scale training, this is critical. Typically you would use a subset of data for these iterations. Once the script is working reliably, you could then use the standard mode for training and hosting which automatically provisions infrastructure on your behalf. Likewise, SageMaker destroys that infrastructure when training is complete, ensuring no charges for idle servers.\n",
    "\n",
    "To train in Local Mode, it is necessary to have docker-compose or nvidia-docker-compose (for GPU) installed in the notebook instance. Running following script will install docker-compose or nvidia-docker-compose and configure the notebook environment for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/bin/bash ./setup.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "\n",
    "from sagemaker.tensorflow import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local = False\n",
    "if (local):\n",
    "    train_instance_type = 'local'\n",
    "    serve_instance_type = 'local'\n",
    "else:\n",
    "    train_instance_type = 'ml.c5.xlarge' \n",
    "    serve_instance_type = 'ml.m4.xlarge'\n",
    "\n",
    "hyperparameters = {'epochs': 35, 'data_dir': '/opt/ml/input/data'}\n",
    "\n",
    "loss_estimator = TensorFlow(entry_point='loss_train.py',\n",
    "                       source_dir='scripts',\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       metric_definitions=[\n",
    "                           {'Name' : 'validation:acc', \n",
    "                            'Regex': '.*step.* - val_acc: (\\\\S+)\\n'},\n",
    "                           {'Name' : 'validation:loss', \n",
    "                            'Regex': '- val_loss: (.*?) '}],\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(), # Pass notebook role to container\n",
    "                       framework_version='1.12',\n",
    "                       py_version='py3',\n",
    "                       base_job_name=USER_PREFIX,\n",
    "                       script_mode=True)\n",
    "\n",
    "# In training script, you have to save the model in 'saved model' format to use TF serving\n",
    "#https://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start the training by using the `fit` method passing in the appropriate definition of the train, test, and val data channels. For local mode training, the training starts immediately. For the non-local mode, you will need to wait several minutes as the infrastructure is launched. Following training, the infrastructure is automatically removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (local):\n",
    "    loss_estimator.fit({'train': f'file://{data_dir}/train',\n",
    "                    'test' : f'file://{data_dir}/test',\n",
    "                    'val'  : f'file://{data_dir}/val'}) \n",
    "else:\n",
    "    # upload the files to the s3 bucket\n",
    "    s3_base = sagemaker_session.upload_data(path=data_dir, \n",
    "                                           bucket=sagemaker_session.default_bucket(),\n",
    "                                           key_prefix='loss')\n",
    "    print(s3_base)\n",
    "    loss_estimator.fit({'train': f'{s3_base}/train',\n",
    "                    'test' : f'{s3_base}/test',\n",
    "                    'val'  : f'{s3_base}/val'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model for real-time inference\n",
    "\n",
    "With the training completed, we now deploy the model to provide real-time inference. This command works in either local mode or stands up its own hosting infrastructure. The type is dictated by the `instance_type` parameter. For the non-local mode, this will take several minutes.\n",
    "\n",
    "When deploying locally, you may get some errors. You can ignore them safely as long as the prediction call works in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_predictor = loss_estimator.deploy(initial_instance_count=1, \n",
    "                                       instance_type=serve_instance_type,\n",
    "                                       endpoint_type='tensorflow-serving')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "\n",
    "Now that the endpoint is deployed, we execute a set of inferences against the testing subset of our data. The identical interface is supported for local and non-local modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = loss_predictor.predict(X_test)\n",
    "print('Results: {}\\n'.format(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we display the results, highlighting which predictions were accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_results = results['predictions']\n",
    "fail_count = 0\n",
    "test_count = len(X_test)\n",
    "for i in range(test_count):\n",
    "    if (tmp_results[i][0] > 0.5):\n",
    "        class_predict = 1\n",
    "    else:\n",
    "        class_predict = 0\n",
    "    if (class_predict == y_test[i]):\n",
    "        result = 'PASS'\n",
    "    else:\n",
    "        result = '*FAIL'\n",
    "        fail_count += 1\n",
    "    print('Result: {:.3f}, Target: {}, Result: {}'.format(tmp_results[i][0], \n",
    "                                                          y_test[i],\n",
    "                                                         result))\n",
    "print('Tests: {}, Fails: {}'.format(test_count, fail_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "We can now get rid of our endpoint. This is important in non-local mode, as you will otherwise  be billed for an idle endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not local:\n",
    "    sagemaker.Session().delete_endpoint(loss_predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After local mode, perform remote training, deployment, and inference\n",
    "\n",
    "The first time through the notebook, try everything in local mode. Once you have completed that successfully, go back to the cell that defines `local` and set that to `False`. Re-run the rest of the notebook to experience non-local mode training, deployment, and inference. Be sure to clean up when complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
